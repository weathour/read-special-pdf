{
  "file_name": "Burns 等 - Weak-to-Strong Generalization  Eliciting Strong Capabilities With Weak Supervision.pdf",
  "generated_at": "2025-07-10 02:41:46",
  "structured_info": {
    "title_cn": "弱监督到强泛化：通过弱监督激发强大能力",
    "title_en": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision",
    "category": "Machine Learning",
    "topics": [
      "AI Alignment",
      "Weak Supervision",
      "Model Generalization",
      "Reinforcement Learning from Human Feedback"
    ],
    "keywords": [
      "Weak-to-Strong Generalization",
      "Superalignment",
      "Model Supervision",
      "AI Alignment",
      "Generalization Techniques"
    ],
    "abstract": "We study an analogy to the problem of humans supervising superhuman models: can weak model supervision elicit the full capabilities of a much stronger model? We test this using pretrained language models from the GPT-4 family on NLP, chess, and reward modeling tasks. We find that strong models finetuned on weak labels consistently outperform their weak supervisors, a phenomenon called weak-to-strong generalization. However, naive finetuning alone fails to recover full strong model capabilities, suggesting techniques like RLHF may scale poorly to superhuman models. Simple methods like an auxiliary confidence loss significantly improve generalization, recovering close to GPT-3.5-level performance on NLP tasks when supervising GPT-4 with GPT-2-level supervision. Our results suggest empirical progress is feasible on aligning superhuman models.",
    "methodology": "Finetune strong pretrained models on labels generated by weak supervisors; evaluate on NLP classification (22 datasets), chess puzzles (predicting optimal moves), and ChatGPT reward modeling tasks. Use Performance Gap Recovered (PGR) metric. Test methods: naive finetuning, auxiliary confidence loss, bootstrapping with intermediate models, and unsupervised generative finetuning.",
    "conclusion": "Strong models generalize beyond weak supervisors but with a significant gap versus ground-truth supervision. Naive finetuning achieves modest PGR (e.g., ~50% in NLP), while simple methods like confidence loss boost PGR to nearly 80% in NLP tasks. Weak-to-strong generalization is tractable, offering a pathway to align superhuman models using weak human supervision.",
    "authors": [
      "Collin Burns",
      "Pavel Izmailov",
      "Jan Hendrik Kirchner",
      "Bowen Baker",
      "Leo Gao",
      "Leopold Aschenbrenner",
      "Yining Chen",
      "Adrien Ecoffet",
      "Manas Joglekar",
      "Jan Leike",
      "Ilya Sutskever",
      "Jeff Wu"
    ],
    "publication_year": "2024",
    "venue": "International Conference on Machine Learning (ICML)",
    "doi": "",
    "bibtex_citation": "Burns_Weak-to-Strong_2024",
    "analysis": {
      "Overview": "Proposes weak-to-strong learning as an analogy for aligning superhuman AI: using weak models to supervise strong models, testing generalization across NLP, chess, and reward tasks. Demonstrates feasibility of eliciting latent capabilities beyond weak supervision.",
      "Background_and_Motivation": [
        "Alignment techniques like RLHF rely on human supervision, but superhuman models will exhibit behaviors too complex for reliable human evaluation.",
        "Motivation: Address how weak supervisors can control much smarter models to prevent misalignment risks in superhuman AI.",
        "Necessity/urgency argued via catastrophic risk potential of unaligned superhuman models; empirical study bridges theoretical frameworks and real-world scalability.",
        "Specific problem (weak supervision eliciting strong capabilities) linked to broader AI alignment challenge by simulating human-superhuman dynamics with model surrogates.",
        "Contributes to Machine Learning, AI Safety, and interdisciplinary fields like scalable oversight and knowledge elicitation."
      ],
      "Conceptual_Framework_and_Innovations": [
        [
          "Weak-to-Strong Generalization: Phenomenon where strong models trained on weak supervision outperform weak supervisors.",
          "Performance Gap Recovered (PGR): Metric quantifying fraction of performance gap between weak and strong ceiling models recovered via weak supervision.",
          "Salience: Ease of eliciting task knowledge from pretrained representations."
        ],
        "Concepts form a causal network: Weak supervision elicits latent knowledge (salience) in strong models, enabling generalization (PGR) beyond supervisor capabilities.",
        "Key assumptions: Strong pretrained models possess latent task knowledge; weak supervision guides rather than teaches; errors are instance-dependent.",
        "Contributes empirical validation and methods to AI alignment knowledge, advancing from theoretical to testable frameworks."
      ],
      "Methodology": [
        "Core: Finetune strong models (GPT-4 family) on weak labels; evaluate via PGR on NLP, chess, and reward tasks. Methods: auxiliary confidence loss, bootstrapping, unsupervised finetuning.",
        "Novelty: First large-scale empirical study of weak-supervisor/strong-student dynamics. Applicability: Methods are simple and model-agnostic. Rationality: Grounded in semi-supervised learning and knowledge elicitation.",
        "Data: 22 NLP datasets (binary classification), chess puzzles from lichess.org, proprietary ChatGPT reward data. Preprocessing: Balanced classes, soft labels. Representativeness: Diverse tasks but reward data is proprietary.",
        "Rigor: Models span 7 compute orders; metrics include accuracy and PGR. Limitations: Disanalogies in imitation saliency and pretraining leakage affect reliability.",
        "Follows robust finetuning/self-supervised paradigms; influences perspective by focusing on representation linearity and overfitting mitigation."
      ],
      "Results": [
        "Key results: Naive finetuning yields positive PGR (median 50% in NLP, 10-40% in chess, ~10% in RM). Confidence loss boosts NLP PGR to 80%; bootstrapping helps chess; generative finetuning improves RM.",
        "Results significant for scalable alignment; reliability confirmed via consistency across tasks. Stability: PGR scales with model size in NLP but inversely in chess."
      ],
      "Argumentation_and_Logic": [
        "Structure: Introduce problem, propose analogy, present methodology/results, analyze limitations, conclude feasibility.",
        "Key steps: Establish disanalogy risks → Show generalization is possible → Address gaps with simple methods → Discuss implications for alignment.",
        "Strengths: Empirical evidence across tasks; weaknesses: Overfitting to weak labels, RM limitations. Rebuttals: Methods reduce overfitting; disanalogies are testable."
      ],
      "Strengths_and_Limitations": [
        "Strengths: Novel framework for superhuman alignment; methods improve generalization simply; broad evaluation.",
        "Methodology boundaries: Poor RM generalization; supervision errors may be easier/harder to imitate than human errors.",
        "Theoretical constraints: Assumes latent knowledge exists; conclusions bounded by disanalogies (e.g., pretraining leakage inflates prompting performance)."
      ],
      "Academic_Discourse_and_Rhetoric": [
        "Positions within discourse: Empirical response to theoretical scalable oversight and ELK (Eliciting Latent Knowledge) problems.",
        "Terminology: Technical (e.g., PGR, salience); tone: Urgent but optimistic; rhetoric: Uses figures/analogies (Figure 1) to simplify high-stakes concepts.",
        "Authority built via citations (Christiano et al., Irving et al.); motivations: Safety focus, open-sourced code, call for community effort."
      ],
      "Conclusions_and_Implications": [
        "Conclusions: Weak-to-strong generalization is feasible and improvable; simple methods recover significant performance, suggesting alignment progress is possible.",
        "Future research: Address disanalogies (e.g., imitation saliency), test generative tasks, combine with scalable oversight techniques, explore linear probes for concept salience."
      ]
    }
  }
}