{
  "file_name": "Li 等 - 2020 - Deep reinforcement learning for pedestrian collision avoidance and human-machine cooperative driving.pdf",
  "generated_at": "2025-07-11 09:35:14",
  "structured_info": {
    "title_cn": "基于深度强化学习的行人防撞与人机协同驾驶",
    "title_en": "Deep reinforcement learning for pedestrian collision avoidance and human-machine cooperative driving",
    "category": "Machine Learning",
    "topics": [
      "Human-Machine Cooperative Driving",
      "Pedestrian Collision Avoidance",
      "Deep Reinforcement Learning"
    ],
    "keywords": [
      "Human-machine cooperative driving",
      "Pedestrian collision avoidance",
      "Deep reinforcement learning",
      "Deep Q-network (DQN)"
    ],
    "abstract": "This paper proposes a learning-based human-machine cooperative driving scheme (L-HMC) with active collision avoidance capacity using deep reinforcement learning. An improved deep Q-network (DQN) method with two nonuniform replay buffers accelerates policy learning for pedestrian collision avoidance. The scheme assists human drivers during dangerous situations by taking control authority when collision risks are detected. Simulations using PreScan with a real vehicle dynamic model demonstrate the method's effectiveness in avoiding collisions through flexible braking or lane-changing strategies across various scenarios.",
    "methodology": "Improved Deep Q-Network (DQN) with dual replay buffers for prioritized experience replay; Markov Decision Process (MDP) formulation for collision avoidance; Human-machine cooperative control switching mechanism based on safety distance thresholds; Simulation validation using PreScan with Audi A8 dynamic model.",
    "conclusion": "The L-HMC scheme successfully avoids pedestrian collisions in 100% of tested scenarios using learned braking or lane-changing policies. The improved DQN achieves faster convergence than standard DQN, and the cooperative system enhances driving safety by intervening during human driver errors.",
    "authors": [
      "Junxiang Li",
      "Liang Yao",
      "Xin Xu",
      "Bang Cheng",
      "Junkai Ren"
    ],
    "publication_year": "2020",
    "venue": "Information Sciences",
    "doi": "https://doi.org/10.1016/j.ins.2020.03.105",
    "bibtex_citation": "Li_Deep_2020",
    "analysis": {
      "Overview": "Proposes a deep reinforcement learning framework for pedestrian collision avoidance within a human-machine cooperative driving system, combining improved DQN algorithms with real-time control authority switching.",
      "Background_and_Motivation": [
        "27,000 annual pedestrian deaths globally (WHO data) primarily caused by driver errors like distraction/unskilled operation.",
        "Limitations of existing ADAS (e.g., rigid braking-only strategies, inability for proactive lane-changing) necessitate adaptive solutions.",
        "Urgency established through traffic death statistics and documented inflexibility of current systems in dynamic scenarios.",
        "Bridges autonomous driving research with human factors engineering to address safety-critical edge cases in semi-autonomous vehicles.",
        "Contributes to machine learning (DRL algorithms), intelligent transportation systems, and human-computer interaction."
      ],
      "Conceptual_Framework_and_Innovations": [
        [
          "Improved DQN: DQN variant with dual replay buffers (general + negative samples) accelerating convergence",
          "L-HMC Scheme: Control architecture switching authority from human to AI agent during collision risks",
          "Safety Trigger: Distance-based mechanism activating cooperative control using t_max (pedestrian crossing time)"
        ],
        "DQN learns avoidance policies → Policies integrated into L-HMC scheme → Safety trigger determines when to activate policies during human driving.",
        "Pedestrian motion follows predictable patterns; Vehicle dynamics accurately modeled; Human drivers may fail to react optimally.",
        "Algorithmic contribution (improved DQN) and applied framework (L-HMC) advancing safe human-AI collaboration in driving."
      ],
      "Methodology": [
        "Improved DQN with prioritized experience replay via two buffers; MDP formulation with 9D state space (relative positions/velocities) and 3 discrete actions (maintain, lane-change, brake).",
        "Novelty: Dual-buffer DQN reduces training time; Applicability: Real-time control via lightweight network; Rationality: MDP design aligns with physical collision dynamics.",
        "Simulated data from PreScan; States: relative vehicle-pedestrian positions/velocities; Preprocessing: Sequential state stacking; Representativeness: Limited to structured two-lane roads with single pedestrians.",
        "Rigorous comparison vs. standard DQN; Metrics: collision rate, convergence speed, success rate; Evaluation adequacy: 100% success but lacks real-world validation.",
        "Follows reinforcement learning (value-based) paradigm, prioritizing empirical optimization over theoretical safety guarantees."
      ],
      "Results": [
        "Improved DQN converges faster than standard DQN; L-HMC achieves 100% collision avoidance across 60 tests; Policies flexibly choose braking (67%) or lane-changing (33%) based on velocity.",
        "Results demonstrate statistical reliability (consistent success) and stability (handled velocity ranges: 10-60 km/h), though limited to simulation environments."
      ],
      "Argumentation_and_Logic": [
        "Problem motivation → Algorithmic solution (DQN) → System integration (L-HMC) → Experimental validation.",
        "1. Frame collision avoidance as MDP; 2. Accelerate learning via dual buffers; 3. Design safety trigger; 4. Validate in simulation.",
        "Strengths: Clear algorithmic innovation and thorough simulation; Weaknesses: Limited scenario complexity; Addressed rebuttals via future work commitments (continuous actions, multi-agent)."
      ],
      "Strengths_and_Limitations": [
        "Novel dual-replay DQN; First DRL application to pedestrian avoidance in HMC; 100% simulated success rate.",
        "Simulation-only validation; Single-pedestrian/two-lane constraints; Discrete actions restrict maneuver complexity.",
        "Value-based RL paradigm limits safety verification; Assumption of predictable pedestrian behavior reduces real-world applicability."
      ],
      "Academic_Discourse_and_Rhetoric": [
        "Positions as bridge between autonomous driving and human factors communities; Advances DRL applications beyond gaming/robotics.",
        "Technical terminology (MDP, replay buffer); Persuasive tone emphasizing safety impact; Strategic citations to WHO (urgency) and seminal DQN work (credibility).",
        "Builds authority through citations of foundational RL works (Mnih et al.) and ADAS literature; Motivations include establishing novelty in HMC domain."
      ],
      "Conclusions_and_Implications": [
        "L-HMC with improved DQN effectively prevents pedestrian collisions; Dual-replay buffers accelerate policy learning; System enables flexible avoidance strategies.",
        "Extend to multi-pedestrian scenarios; Implement continuous action spaces; Address complex human behaviors; Pursue real-world testing and transfer learning."
      ]
    }
  }
}