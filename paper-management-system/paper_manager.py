
# paper_manager.py - ä¿®å¤å¹´ä»½æ•°æ®ç±»å‹é—®é¢˜

import os
import json
import sqlite3
import re
from pathlib import Path
from datetime import datetime
from flask import Flask, render_template, request, jsonify, redirect, url_for, session
from werkzeug.utils import secure_filename
import markdown
from collections import defaultdict, Counter
import os
from flask import send_from_directory, abort
from urllib.parse import quote

app = Flask(__name__)
app.secret_key = 'paper-manager-secret-key'
app.config['JSON_FOLDER'] = 'json_papers'  # JSONæ–‡ä»¶å­˜å‚¨æ–‡ä»¶å¤¹
app.config['DATABASE'] = 'papers.db'

# é…ç½®PDFæ–‡ä»¶å¤¹è·¯å¾„
PDF_FOLDER = 'pdfs'
# ç¡®ä¿å¿…è¦çš„æ–‡ä»¶å¤¹å­˜åœ¨
os.makedirs(app.config['JSON_FOLDER'], exist_ok=True)

class PaperManager:
    """è®ºæ–‡ç®¡ç†æ ¸å¿ƒç±»"""
    
    def __init__(self, db_path='papers.db'):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºè®ºæ–‡è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS papers (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_name TEXT NOT NULL,
                title_cn TEXT,
                title_en TEXT,
                category TEXT,
                topics TEXT,  -- JSONæ ¼å¼å­˜å‚¨
                keywords TEXT,  -- JSONæ ¼å¼å­˜å‚¨
                abstract TEXT,
                methodology TEXT,
                conclusion TEXT,
                authors TEXT,  -- JSONæ ¼å¼å­˜å‚¨
                publication_year INTEGER,
                venue TEXT,
                doi TEXT,
                bibtex_citation TEXT,
                analysis TEXT,  -- JSONæ ¼å¼å­˜å‚¨å®Œæ•´åˆ†æ
                json_path TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_title_cn ON papers(title_cn)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_title_en ON papers(title_en)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_category ON papers(category)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_year ON papers(publication_year)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_venue ON papers(venue)')
        
        conn.commit()
        conn.close()
    
    def load_json_file(self, json_path):
        """åŠ è½½JSONæ–‡ä»¶"""
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"åŠ è½½JSONæ–‡ä»¶å¤±è´¥: {json_path}, é”™è¯¯: {e}")
            return None
    
    def parse_structured_info(self, structured_info_str):
        """è§£æç»“æ„åŒ–ä¿¡æ¯å­—ç¬¦ä¸²"""
        try:
            if isinstance(structured_info_str, str):
                return json.loads(structured_info_str)
            elif isinstance(structured_info_str, dict):
                return structured_info_str
            else:
                return {}
        except Exception as e:
            print(f"è§£æstructured_infoå¤±è´¥: {e}")
            return {}
    
    def convert_indexed_dict_to_list(self, indexed_dict):
        """å°†ç´¢å¼•å­—å…¸è½¬æ¢ä¸ºåˆ—è¡¨"""
        if not indexed_dict:
            return []
        
        if isinstance(indexed_dict, list):
            return indexed_dict
        
        if isinstance(indexed_dict, dict):
            # å¦‚æœæ˜¯ç´¢å¼•å­—å…¸ {"0": "value1", "1": "value2"}
            try:
                # å°è¯•æŒ‰æ•°å­—ç´¢å¼•æ’åº
                max_key = max(int(k) for k in indexed_dict.keys() if k.isdigit())
                result = []
                for i in range(max_key + 1):
                    if str(i) in indexed_dict:
                        result.append(indexed_dict[str(i)])
                return result
            except:
                # å¦‚æœä¸æ˜¯æ•°å­—ç´¢å¼•ï¼Œè¿”å›æ‰€æœ‰å€¼
                return list(indexed_dict.values())
        
        return []
    
    def safe_int(self, value, default=0):
        """å®‰å…¨åœ°è½¬æ¢ä¸ºæ•´æ•°"""
        if value is None:
            return default
        try:
            if isinstance(value, str):
                # å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„å¹´ä»½
                value = value.strip()
                if not value:
                    return default
                # åªå–æ•°å­—éƒ¨åˆ†
                import re
                match = re.search(r'\d{4}', value)
                if match:
                    return int(match.group())
                return default
            return int(value)
        except (ValueError, TypeError):
            return default
    
    def import_json_file(self, json_path):
        """å¯¼å…¥å•ä¸ªJSONæ–‡ä»¶åˆ°æ•°æ®åº“"""
        data = self.load_json_file(json_path)
        if not data:
            return False
        
        # è§£æç»“æ„åŒ–ä¿¡æ¯
        structured_info = self.parse_structured_info(data.get('structured_info', '{}'))
        
        # å¤„ç† topics (å¯èƒ½æ˜¯ç´¢å¼•å­—å…¸)
        topics_raw = structured_info.get('topics', [])
        topics_list = self.convert_indexed_dict_to_list(topics_raw)
        
        # å¤„ç† keywords (å¯èƒ½æ˜¯ç´¢å¼•å­—å…¸)
        keywords_raw = structured_info.get('keywords', [])
        keywords_list = self.convert_indexed_dict_to_list(keywords_raw)
        
        # å¤„ç† authors (å¯èƒ½æ˜¯ç´¢å¼•å­—å…¸)
        authors_raw = structured_info.get('authors', [])
        authors_list = self.convert_indexed_dict_to_list(authors_raw)
        
        # å¤„ç† analysis (å¯èƒ½æ˜¯å­—ç¬¦ä¸²)
        analysis_raw = structured_info.get('analysis', {})
        if isinstance(analysis_raw, str):
            try:
                analysis_dict = json.loads(analysis_raw)
            except:
                analysis_dict = {}
        else:
            analysis_dict = analysis_raw
        
        # å®‰å…¨å¤„ç†å¹´ä»½
        publication_year = self.safe_int(structured_info.get('publication_year', 0))
        
        # æå–ä¿¡æ¯
        paper_data = {
            'file_name': data.get('file_name', ''),
            'title_cn': structured_info.get('title_cn', ''),
            'title_en': structured_info.get('title_en', ''),
            'category': structured_info.get('category', ''),
            'topics': json.dumps(topics_list, ensure_ascii=False),
            'keywords': json.dumps(keywords_list, ensure_ascii=False),
            'abstract': structured_info.get('abstract', ''),
            'methodology': structured_info.get('methodology', ''),
            'conclusion': structured_info.get('conclusion', ''),
            'authors': json.dumps(authors_list, ensure_ascii=False),
            'publication_year': publication_year,
            'venue': structured_info.get('venue', ''),
            'doi': structured_info.get('doi', ''),
            'bibtex_citation': structured_info.get('bibtex_citation', ''),
            'analysis': json.dumps(analysis_dict, ensure_ascii=False),
            'json_path': str(json_path)
        }
        
        # æ’å…¥æ•°æ®åº“
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        cursor.execute('SELECT id FROM papers WHERE json_path = ?', (str(json_path),))
        existing = cursor.fetchone()
        
        if existing:
            # æ›´æ–°ç°æœ‰è®°å½•
            cursor.execute('''
                UPDATE papers SET 
                    file_name=?, title_cn=?, title_en=?, category=?, topics=?, keywords=?,
                    abstract=?, methodology=?, conclusion=?, authors=?, publication_year=?,
                    venue=?, doi=?, bibtex_citation=?, analysis=?, updated_at=CURRENT_TIMESTAMP
                WHERE json_path=?
            ''', (
                paper_data['file_name'], paper_data['title_cn'], paper_data['title_en'],
                paper_data['category'], paper_data['topics'], paper_data['keywords'],
                paper_data['abstract'], paper_data['methodology'], paper_data['conclusion'],
                paper_data['authors'], paper_data['publication_year'], paper_data['venue'],
                paper_data['doi'], paper_data['bibtex_citation'], paper_data['analysis'],
                str(json_path)
            ))
        else:
            # æ’å…¥æ–°è®°å½•
            cursor.execute('''
                INSERT INTO papers (
                    file_name, title_cn, title_en, category, topics, keywords,
                    abstract, methodology, conclusion, authors, publication_year,
                    venue, doi, bibtex_citation, analysis, json_path
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                paper_data['file_name'], paper_data['title_cn'], paper_data['title_en'],
                paper_data['category'], paper_data['topics'], paper_data['keywords'],
                paper_data['abstract'], paper_data['methodology'], paper_data['conclusion'],
                paper_data['authors'], paper_data['publication_year'], paper_data['venue'],
                paper_data['doi'], paper_data['bibtex_citation'], paper_data['analysis'],
                str(json_path)
            ))
        
        conn.commit()
        conn.close()
        return True
    
    def import_json_folder(self, folder_path):
        """æ‰¹é‡å¯¼å…¥JSONæ–‡ä»¶å¤¹"""
        folder_path = Path(folder_path)
        if not folder_path.exists():
            return 0, [f"è·¯å¾„ä¸å­˜åœ¨: {folder_path}"]
        
        json_files = list(folder_path.rglob('*.json'))
        if not json_files:
            return 0, [f"åœ¨è·¯å¾„ {folder_path} ä¸­æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶"]
        
        imported_count = 0
        failed_files = []
        
        print(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
        
        for json_file in json_files:
            try:
                print(f"æ­£åœ¨å¯¼å…¥: {json_file}")
                if self.import_json_file(json_file):
                    imported_count += 1
                    print(f"æˆåŠŸå¯¼å…¥: {json_file}")
                else:
                    failed_files.append(str(json_file))
                    print(f"å¯¼å…¥å¤±è´¥: {json_file}")
            except Exception as e:
                failed_files.append(f"{json_file}: {e}")
                print(f"å¯¼å…¥å¼‚å¸¸: {json_file}: {e}")
        
        print(f"å¯¼å…¥å®Œæˆ: {imported_count} ä¸ªæˆåŠŸ, {len(failed_files)} ä¸ªå¤±è´¥")
        return imported_count, failed_files
    
    def check_pdf_exists(self, pdf_filename):
        """æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        if not pdf_filename:
            return False
        
        # å¦‚æœæ–‡ä»¶åä»¥.jsonç»“å°¾ï¼Œæ›¿æ¢ä¸º.pdf
        if pdf_filename.endswith('.json'):
            pdf_filename = pdf_filename[:-5] + '.pdf'
        elif not pdf_filename.endswith('.pdf'):
            pdf_filename += '.pdf'
        
        pdf_path = os.path.join(PDF_FOLDER, pdf_filename)
        return os.path.exists(pdf_path)

    def get_pdf_filename(self, json_filename):
        """ä»JSONæ–‡ä»¶åè·å–PDFæ–‡ä»¶å"""
        if not json_filename:
            return None
        
        # å¦‚æœæ–‡ä»¶åä»¥.jsonç»“å°¾ï¼Œæ›¿æ¢ä¸º.pdf
        if json_filename.endswith('.json'):
            pdf_filename = json_filename[:-5] + '.pdf'
        elif not json_filename.endswith('.pdf'):
            pdf_filename = json_filename + '.pdf'
        else:
            pdf_filename = json_filename
        
        return pdf_filename




    def search_papers(self, query='', category='', author='', venue='', year_range=None, limit=50, offset=0):
        """æœç´¢è®ºæ–‡ - å¢å¼ºç‰ˆ (å¢åŠ æœŸåˆŠæœç´¢)"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        conditions = []
        params = []
        
        # å…³é”®è¯æœç´¢
        if query:
            conditions.append('''
                (title_cn LIKE ? OR title_en LIKE ? OR abstract LIKE ? OR 
                keywords LIKE ? OR authors LIKE ? OR venue LIKE ? OR methodology LIKE ? OR conclusion LIKE ?)
            ''')
            query_param = f'%{query}%'
            params.extend([query_param] * 8)
        
        # åˆ†ç±»æœç´¢
        if category:
            conditions.append('category = ?')
            params.append(category)
        
        # ä½œè€…æœç´¢
        if author:
            conditions.append('authors LIKE ?')
            params.append(f'%{author}%')
        
        # æœŸåˆŠæœç´¢ - æ–°å¢
        if venue:
            conditions.append('venue = ?')
            params.append(venue)
        
        # å¹´ä»½èŒƒå›´
        if year_range:
            if year_range[0]:
                conditions.append('publication_year >= ?')
                params.append(year_range[0])
            if year_range[1]:
                conditions.append('publication_year <= ?')
                params.append(year_range[1])
        
        where_clause = ' AND '.join(conditions) if conditions else '1=1'
        
        # è·å–æ€»æ•°
        cursor.execute(f'SELECT COUNT(*) FROM papers WHERE {where_clause}', params)
        total_count = cursor.fetchone()[0]
        
        # è·å–ç»“æœ
        cursor.execute(f'''
            SELECT * FROM papers WHERE {where_clause}
            ORDER BY publication_year DESC, title_en
            LIMIT ? OFFSET ?
        ''', params + [limit, offset])
        
        papers = [dict(row) for row in cursor.fetchall()]
        
        # è§£æä½œè€…ä¿¡æ¯
        for paper in papers:
            try:
                paper['authors_list'] = json.loads(paper['authors']) if paper['authors'] else []
            except:
                paper['authors_list'] = []
        
        conn.close()
        
        return papers, total_count
    

    def parse_analysis_content(self, analysis_data):
        """è§£æåˆ†æå†…å®¹ï¼Œå¤„ç†ä¸åŒæ•°æ®ç»“æ„"""
        if not analysis_data:
            return {}
        
        parsed_analysis = {}
        
        for key, value in analysis_data.items():
            # æ ¼å¼åŒ–æ ‡é¢˜
            formatted_key = self.format_analysis_title(key)
            
            # å¤„ç†ä¸åŒç±»å‹çš„å€¼
            if isinstance(value, str):
                # å­—ç¬¦ä¸²ç›´æ¥ä½¿ç”¨
                parsed_analysis[formatted_key] = {
                    'type': 'text',
                    'content': value
                }
            elif isinstance(value, list):
                # å¤„ç†åˆ—è¡¨
                parsed_analysis[formatted_key] = self.parse_list_content(value)
            else:
                # å…¶ä»–ç±»å‹è½¬ä¸ºå­—ç¬¦ä¸²
                parsed_analysis[formatted_key] = {
                    'type': 'text',
                    'content': str(value)
                }
        
        return parsed_analysis

    def format_analysis_title(self, title):
        """æ ¼å¼åŒ–åˆ†ææ ‡é¢˜"""
        # å°†ä¸‹åˆ’çº¿æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œå¹¶é¦–å­—æ¯å¤§å†™
        formatted = title.replace('_', ' ').replace('-', ' ')
        # æ¯ä¸ªå•è¯é¦–å­—æ¯å¤§å†™
        formatted = ' '.join(word.capitalize() for word in formatted.split())
        return formatted

    def parse_list_content(self, content_list):
        """è§£æåˆ—è¡¨å†…å®¹"""
        if not content_list:
            return {'type': 'text', 'content': ''}
        
        parsed_items = []
        
        for item in content_list:
            if isinstance(item, str):
                # å­—ç¬¦ä¸²é¡¹
                parsed_items.append({
                    'type': 'text',
                    'content': item
                })
            elif isinstance(item, list):
                # åµŒå¥—åˆ—è¡¨
                if len(item) == 1:
                    # å•é¡¹åˆ—è¡¨
                    parsed_items.append({
                        'type': 'text',
                        'content': str(item[0])
                    })
                else:
                    # å¤šé¡¹åˆ—è¡¨ï¼Œä½œä¸ºå­åˆ—è¡¨
                    parsed_items.append({
                        'type': 'sublist',
                        'content': item
                    })
            else:
                # å…¶ä»–ç±»å‹
                parsed_items.append({
                    'type': 'text',
                    'content': str(item)
                })
        
        return {
            'type': 'list',
            'items': parsed_items
        }





    def get_all_venues(self):
        """è·å–æ‰€æœ‰æœŸåˆŠ/ä¼šè®®åˆ—è¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT DISTINCT venue FROM papers 
            WHERE venue != '' AND venue IS NOT NULL
            ORDER BY venue
        ''')
        venues = [row[0] for row in cursor.fetchall()]
        
        conn.close()
        return venues

    def get_papers_by_venue(self, venue_name, limit=50, offset=0):
        """æ ¹æ®æœŸåˆŠè·å–è®ºæ–‡"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT COUNT(*) FROM papers 
            WHERE venue = ?
        ''', (venue_name,))
        total_count = cursor.fetchone()[0]
        
        cursor.execute('''
            SELECT * FROM papers 
            WHERE venue = ?
            ORDER BY publication_year DESC, title_en
            LIMIT ? OFFSET ?
        ''', (venue_name, limit, offset))
        
        papers = [dict(row) for row in cursor.fetchall()]
        
        # è§£æä½œè€…ä¿¡æ¯
        for paper in papers:
            try:
                paper['authors_list'] = json.loads(paper['authors']) if paper['authors'] else []
            except:
                paper['authors_list'] = []
        
        conn.close()
        return papers, total_count

    def get_venue_statistics(self):
        """è·å–æœŸåˆŠç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT venue, COUNT(*) as count 
            FROM papers 
            WHERE venue != '' AND venue IS NOT NULL
            GROUP BY venue 
            ORDER BY count DESC, venue
        ''')
        
        venue_stats = cursor.fetchall()
        conn.close()
        
        return venue_stats










    def get_all_authors(self):
        """è·å–æ‰€æœ‰ä½œè€…åˆ—è¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT DISTINCT authors FROM papers WHERE authors != ""')
        authors_json_list = cursor.fetchall()
        
        all_authors = set()
        for (authors_json,) in authors_json_list:
            try:
                authors = json.loads(authors_json)
                if isinstance(authors, list):
                    for author in authors:
                        if author.strip():
                            all_authors.add(author.strip())
            except:
                continue
        
        conn.close()
        return sorted(list(all_authors))

    def get_papers_by_author(self, author_name, limit=50, offset=0):
        """æ ¹æ®ä½œè€…è·å–è®ºæ–‡"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT COUNT(*) FROM papers 
            WHERE authors LIKE ?
        ''', (f'%{author_name}%',))
        total_count = cursor.fetchone()[0]
        
        cursor.execute('''
            SELECT * FROM papers 
            WHERE authors LIKE ?
            ORDER BY publication_year DESC, title_en
            LIMIT ? OFFSET ?
        ''', (f'%{author_name}%', limit, offset))
        
        papers = [dict(row) for row in cursor.fetchall()]
        
        # è§£æä½œè€…ä¿¡æ¯
        for paper in papers:
            try:
                paper['authors_list'] = json.loads(paper['authors']) if paper['authors'] else []
            except:
                paper['authors_list'] = []
        
        conn.close()
        return papers, total_count



    def find_duplicate_papers(self):
        """æŸ¥æ‰¾é‡å¤è®ºæ–‡ï¼ˆåŸºäºDOIï¼‰"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æŸ¥æ‰¾æœ‰ç›¸åŒDOIçš„è®ºæ–‡ç»„
        cursor.execute('''
            SELECT doi, COUNT(*) as count, GROUP_CONCAT(id) as ids
            FROM papers 
            WHERE doi != '' AND doi IS NOT NULL
            GROUP BY doi 
            HAVING COUNT(*) > 1
            ORDER BY count DESC
        ''')
        
        duplicates = []
        for row in cursor.fetchall():
            doi, count, ids_str = row
            ids = [int(id) for id in ids_str.split(',')]
            
            # è·å–è¿™äº›é‡å¤è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯
            cursor.execute('''
                SELECT id, title_cn, title_en, file_name, created_at, updated_at
                FROM papers 
                WHERE id IN ({})
                ORDER BY updated_at DESC
            '''.format(','.join(['?'] * len(ids))), ids)
            
            papers = cursor.fetchall()
            duplicates.append({
                'doi': doi,
                'count': count,
                'papers': papers
            })
        
        conn.close()
        return duplicates

    def find_duplicate_papers_by_title(self):
        """åŸºäºæ ‡é¢˜æŸ¥æ‰¾å¯èƒ½çš„é‡å¤è®ºæ–‡"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æŸ¥æ‰¾æœ‰ç›¸åŒæ ‡é¢˜çš„è®ºæ–‡
        cursor.execute('''
            SELECT title_en, COUNT(*) as count, GROUP_CONCAT(id) as ids
            FROM papers 
            WHERE title_en != '' AND title_en IS NOT NULL
            GROUP BY LOWER(title_en)
            HAVING COUNT(*) > 1
            ORDER BY count DESC
        ''')
        
        duplicates = []
        for row in cursor.fetchall():
            title, count, ids_str = row
            ids = [int(id) for id in ids_str.split(',')]
            
            cursor.execute('''
                SELECT id, title_cn, title_en, file_name, doi, created_at, updated_at
                FROM papers 
                WHERE id IN ({})
                ORDER BY updated_at DESC
            '''.format(','.join(['?'] * len(ids))), ids)
            
            papers = cursor.fetchall()
            duplicates.append({
                'title': title,
                'count': count,
                'papers': papers
            })
        
        conn.close()
        return duplicates

    def auto_deduplicate_by_doi(self):
        """è‡ªåŠ¨å»é‡ï¼šåŸºäºDOIï¼Œä¿ç•™æœ€æ–°ç‰ˆæœ¬"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        removed_count = 0
        duplicates = self.find_duplicate_papers()
        
        for duplicate in duplicates:
            papers = duplicate['papers']
            if len(papers) > 1:
                # ä¿ç•™æœ€æ–°çš„ï¼ˆç¬¬ä¸€ä¸ªï¼Œå› ä¸ºå·²ç»æŒ‰updated_at DESCæ’åºï¼‰
                keep_paper = papers[0]
                remove_papers = papers[1:]
                
                # åˆ é™¤æ—§ç‰ˆæœ¬
                for paper in remove_papers:
                    cursor.execute('DELETE FROM papers WHERE id = ?', (paper[0],))
                    removed_count += 1
                    print(f"åˆ é™¤é‡å¤è®ºæ–‡: {paper[3]} (ID: {paper[0]})")
        
        conn.commit()
        conn.close()
        
        return removed_count

    def merge_paper_data(self, keep_id, remove_id):
        """åˆå¹¶è®ºæ–‡æ•°æ®ï¼ˆä¿ç•™è¾ƒå®Œæ•´çš„æ•°æ®ï¼‰"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # è·å–ä¸¤ç¯‡è®ºæ–‡çš„æ•°æ®
        cursor.execute('SELECT * FROM papers WHERE id = ?', (keep_id,))
        keep_paper = cursor.fetchone()
        
        cursor.execute('SELECT * FROM papers WHERE id = ?', (remove_id,))
        remove_paper = cursor.fetchone()
        
        if not keep_paper or not remove_paper:
            conn.close()
            return False
        
        # åˆå¹¶æ•°æ®çš„é€»è¾‘ï¼ˆé€‰æ‹©éç©ºçš„å­—æ®µï¼‰
        merged_data = {}
        columns = [desc[0] for desc in cursor.description]
        
        for i, column in enumerate(columns):
            if column == 'id':
                merged_data[column] = keep_paper[i]
            elif column in ['created_at', 'updated_at']:
                # ä¿ç•™è¾ƒæ–°çš„æ—¶é—´
                merged_data[column] = max(keep_paper[i], remove_paper[i])
            else:
                # é€‰æ‹©éç©ºçš„å€¼ï¼Œå¦‚æœéƒ½éç©ºåˆ™ä¿ç•™keep_paperçš„å€¼
                keep_value = keep_paper[i]
                remove_value = remove_paper[i]
                
                if not keep_value and remove_value:
                    merged_data[column] = remove_value
                else:
                    merged_data[column] = keep_value
        
        # æ›´æ–°ä¿ç•™çš„è®ºæ–‡
        update_fields = [f"{col} = ?" for col in columns if col != 'id']
        update_values = [merged_data[col] for col in columns if col != 'id']
        update_values.append(keep_id)
        
        cursor.execute(f'''
            UPDATE papers SET {', '.join(update_fields)}
            WHERE id = ?
        ''', update_values)
        
        # åˆ é™¤è¦ç§»é™¤çš„è®ºæ–‡
        cursor.execute('DELETE FROM papers WHERE id = ?', (remove_id,))
        
        conn.commit()
        conn.close()
        
        return True

    def get_deduplication_stats(self):
        """è·å–å»é‡ç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        stats = {}
        
        # æ€»è®ºæ–‡æ•°
        cursor.execute('SELECT COUNT(*) FROM papers')
        stats['total_papers'] = cursor.fetchone()[0]
        
        # æœ‰DOIçš„è®ºæ–‡æ•°
        cursor.execute('SELECT COUNT(*) FROM papers WHERE doi != "" AND doi IS NOT NULL')
        stats['papers_with_doi'] = cursor.fetchone()[0]
        
        # DOIé‡å¤çš„è®ºæ–‡ç»„æ•°
        cursor.execute('''
            SELECT COUNT(*) FROM (
                SELECT doi FROM papers 
                WHERE doi != "" AND doi IS NOT NULL
                GROUP BY doi 
                HAVING COUNT(*) > 1
            )
        ''')
        stats['duplicate_doi_groups'] = cursor.fetchone()[0]
        
        # æ ‡é¢˜é‡å¤çš„è®ºæ–‡ç»„æ•°
        cursor.execute('''
            SELECT COUNT(*) FROM (
                SELECT title_en FROM papers 
                WHERE title_en != "" AND title_en IS NOT NULL
                GROUP BY LOWER(title_en)
                HAVING COUNT(*) > 1
            )
        ''')
        stats['duplicate_title_groups'] = cursor.fetchone()[0]
        
        conn.close()
        return stats




    def get_paper_by_id(self, paper_id):
        """æ ¹æ®IDè·å–è®ºæ–‡è¯¦æƒ…"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM papers WHERE id = ?', (paper_id,))
        paper = cursor.fetchone()
        conn.close()
        
        if paper:
            return dict(paper)
        return None
    

    def get_statistics(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯ - å¢å¼ºç‰ˆ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        stats = {}
        
        # æ€»è®ºæ–‡æ•°
        cursor.execute('SELECT COUNT(*) FROM papers')
        stats['total_papers'] = cursor.fetchone()[0]
        
        # æŒ‰å¹´ä»½ç»Ÿè®¡
        cursor.execute('''
            SELECT publication_year, COUNT(*) as count 
            FROM papers 
            WHERE publication_year > 0 
            GROUP BY publication_year 
            ORDER BY publication_year DESC
        ''')
        year_results = cursor.fetchall()
        stats['by_year'] = {}
        for year, count in year_results:
            stats['by_year'][str(year)] = count
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        cursor.execute('''
            SELECT category, COUNT(*) as count 
            FROM papers 
            WHERE category != '' 
            GROUP BY category 
            ORDER BY count DESC
        ''')
        category_results = cursor.fetchall()
        stats['by_category'] = dict(category_results)
        stats['top_categories'] = category_results[:10]
        
        # æŒ‰æœŸåˆŠç»Ÿè®¡ - å¢å¼ºç‰ˆ
        cursor.execute('''
            SELECT venue, COUNT(*) as count 
            FROM papers 
            WHERE venue != '' AND venue IS NOT NULL
            GROUP BY venue 
            ORDER BY count DESC
        ''')
        venue_results = cursor.fetchall()
        stats['by_venue'] = dict(venue_results)
        stats['top_venues'] = venue_results[:15]  # æ˜¾ç¤ºå‰15ä¸ªæœŸåˆŠ
        
        # ä¸ºæ¨¡æ¿å‡†å¤‡å‰10ä¸ªå¹´ä»½
        stats['top_years'] = [(str(year), count) for year, count in year_results[:10]]
        
        conn.close()
        return stats

# å…¨å±€å®ä¾‹
paper_manager = PaperManager()

@app.route('/')
def index():
    """ä¸»é¡µ"""
    try:
        stats = paper_manager.get_statistics()
        return render_template('index.html', stats=stats)
    except Exception as e:
        print(f"ä¸»é¡µé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return f"ä¸»é¡µåŠ è½½é”™è¯¯: {e}", 500


# ä¿®æ”¹æœç´¢è·¯ç”±
@app.route('/search')
def search():
    """æœç´¢é¡µé¢ - å¢å¼ºç‰ˆ"""
    query = request.args.get('q', '')
    category = request.args.get('category', '')
    author = request.args.get('author', '')
    venue = request.args.get('venue', '')  # æ–°å¢
    year_start = request.args.get('year_start', '')
    year_end = request.args.get('year_end', '')
    page = int(request.args.get('page', 1))
    per_page = int(request.args.get('per_page', 20))
    
    # å¤„ç†å¹´ä»½èŒƒå›´
    year_range = [None, None]
    if year_start:
        try:
            year_range[0] = int(year_start)
        except ValueError:
            pass
    if year_end:
        try:
            year_range[1] = int(year_end)
        except ValueError:
            pass
    
    # æœç´¢
    offset = (page - 1) * per_page
    papers, total_count = paper_manager.search_papers(
        query=query,
        category=category,
        author=author,
        venue=venue,  # æ–°å¢
        year_range=year_range,
        limit=per_page,
        offset=offset
    )
    
    # åˆ†é¡µä¿¡æ¯
    total_pages = (total_count + per_page - 1) // per_page
    
    return render_template('search.html',
                         papers=papers,
                         total_count=total_count,
                         page=page,
                         total_pages=total_pages,
                         query=query,
                         category=category,
                         author=author,
                         venue=venue,  # æ–°å¢
                         year_start=year_start,
                         year_end=year_end)

@app.route('/venue/<venue_name>')
def venue_papers(venue_name):
    """æœŸåˆŠè®ºæ–‡é¡µé¢"""
    page = int(request.args.get('page', 1))
    per_page = int(request.args.get('per_page', 20))
    
    offset = (page - 1) * per_page
    papers, total_count = paper_manager.get_papers_by_venue(
        venue_name=venue_name,
        limit=per_page,
        offset=offset
    )
    
    total_pages = (total_count + per_page - 1) // per_page
    
    return render_template('venue_papers.html',
                         papers=papers,
                         total_count=total_count,
                         page=page,
                         total_pages=total_pages,
                         venue_name=venue_name)

@app.route('/api/venue_suggestions')
def api_venue_suggestions():
    """API: æœŸåˆŠæœç´¢å»ºè®®"""
    query = request.args.get('q', '').strip()
    if not query or len(query) < 2:
        return jsonify([])
    
    all_venues = paper_manager.get_all_venues()
    suggestions = [venue for venue in all_venues if query.lower() in venue.lower()]
    
    return jsonify(suggestions[:10])


@app.route('/author/<author_name>')
def author_papers(author_name):
    """ä½œè€…è®ºæ–‡é¡µé¢"""
    page = int(request.args.get('page', 1))
    per_page = int(request.args.get('per_page', 20))
    
    offset = (page - 1) * per_page
    papers, total_count = paper_manager.get_papers_by_author(
        author_name=author_name,
        limit=per_page,
        offset=offset
    )
    
    total_pages = (total_count + per_page - 1) // per_page
    
    return render_template('author_papers.html',
                         papers=papers,
                         total_count=total_count,
                         page=page,
                         total_pages=total_pages,
                         author_name=author_name)

@app.route('/api/authors')
def api_authors():
    """API: è·å–æ‰€æœ‰ä½œè€…"""
    authors = paper_manager.get_all_authors()
    return jsonify(authors)

@app.route('/api/author_suggestions')
def api_author_suggestions():
    """API: ä½œè€…æœç´¢å»ºè®®"""
    query = request.args.get('q', '').strip()
    if not query or len(query) < 2:
        return jsonify([])
    
    all_authors = paper_manager.get_all_authors()
    suggestions = [author for author in all_authors if query.lower() in author.lower()]
    
    return jsonify(suggestions[:10])



@app.route('/paper/<int:paper_id>')
def paper_detail(paper_id):
    """è®ºæ–‡è¯¦æƒ…é¡µ - å¢åŠ PDFé“¾æ¥"""
    paper = paper_manager.get_paper_by_id(paper_id)
    if not paper:
        return "è®ºæ–‡ä¸å­˜åœ¨", 404
    
    # è§£æJSONå­—æ®µ
    try:
        paper['topics'] = json.loads(paper['topics']) if paper['topics'] else []
        paper['keywords'] = json.loads(paper['keywords']) if paper['keywords'] else []
        paper['authors'] = json.loads(paper['authors']) if paper['authors'] else []
        
        # è§£æåˆ†æå†…å®¹
        analysis_raw = paper.get('analysis', '')
        if isinstance(analysis_raw, str):
            if analysis_raw.strip():
                try:
                    paper['analysis'] = json.loads(analysis_raw)
                except json.JSONDecodeError:
                    paper['analysis'] = {}
            else:
                paper['analysis'] = {}
        elif isinstance(analysis_raw, dict):
            paper['analysis'] = analysis_raw
        else:
            paper['analysis'] = {}
        
        # æ·»åŠ PDFä¿¡æ¯
        paper['pdf_filename'] = paper_manager.get_pdf_filename(paper['file_name'])
        paper['pdf_exists'] = paper_manager.check_pdf_exists(paper['file_name'])
        
        paper['parsed_analysis'] = None
        
    except Exception as e:
        print(f"è§£æè®ºæ–‡æ•°æ®å¤±è´¥: {e}")
        paper['topics'] = []
        paper['keywords'] = []
        paper['authors'] = []
        paper['analysis'] = {}
        paper['parsed_analysis'] = None
        paper['pdf_filename'] = None
        paper['pdf_exists'] = False
    
    return render_template('paper_detail.html', paper=paper)

# æ·»åŠ PDFæ–‡ä»¶å¤¹çŠ¶æ€æ£€æŸ¥çš„API
@app.route('/api/pdf-status')
def pdf_status():
    """æ£€æŸ¥PDFæ–‡ä»¶å¤¹çŠ¶æ€"""
    try:
        if not os.path.exists(PDF_FOLDER):
            return jsonify({
                'status': 'folder_not_exists',
                'message': 'PDFæ–‡ä»¶å¤¹ä¸å­˜åœ¨',
                'pdf_count': 0
            })
        
        pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]
        
        return jsonify({
            'status': 'ok',
            'message': f'PDFæ–‡ä»¶å¤¹æ­£å¸¸ï¼Œå…±{len(pdf_files)}ä¸ªæ–‡ä»¶',
            'pdf_count': len(pdf_files),
            'pdf_files': pdf_files
        })
    
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': str(e),
            'pdf_count': 0
        })








# æ·»åŠ PDFæœåŠ¡è·¯ç”±
@app.route('/pdf/<filename>')
def serve_pdf(filename):
    """æä¾›PDFæ–‡ä»¶ä¸‹è½½/æŸ¥çœ‹"""
    try:
        # ç¡®ä¿æ–‡ä»¶åå®‰å…¨
        filename = os.path.basename(filename)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        pdf_path = os.path.join(PDF_FOLDER, filename)
        if not os.path.exists(pdf_path):
            abort(404)
        
        # è¿”å›æ–‡ä»¶ï¼Œæ”¯æŒåœ¨çº¿é¢„è§ˆ
        return send_from_directory(PDF_FOLDER, filename, as_attachment=False)
    
    except Exception as e:
        print(f"PDFæœåŠ¡é”™è¯¯: {e}")
        abort(404)

@app.route('/pdf/<filename>/download')
def download_pdf(filename):
    """ä¸‹è½½PDFæ–‡ä»¶"""
    try:
        # ç¡®ä¿æ–‡ä»¶åå®‰å…¨
        filename = os.path.basename(filename)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        pdf_path = os.path.join(PDF_FOLDER, filename)
        if not os.path.exists(pdf_path):
            abort(404)
        
        # è¿”å›æ–‡ä»¶ä½œä¸ºé™„ä»¶ä¸‹è½½
        return send_from_directory(PDF_FOLDER, filename, as_attachment=True)
    
    except Exception as e:
        print(f"PDFä¸‹è½½é”™è¯¯: {e}")
        abort(404)
    
    return render_template('paper_detail.html', paper=paper)

@app.route('/import', methods=['GET', 'POST'])
def import_papers():
    """å¯¼å…¥è®ºæ–‡"""
    if request.method == 'POST':
        folder_path = request.form.get('folder_path', '').strip()
        
        if not folder_path:
            return render_template('import.html', error='è¯·è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„')
        
        try:
            imported_count, failed_files = paper_manager.import_json_folder(folder_path)
            
            return render_template('import.html',
                                 success=f'æˆåŠŸå¯¼å…¥ {imported_count} ä¸ªè®ºæ–‡',
                                 failed_files=failed_files)
        except Exception as e:
            return render_template('import.html', error=f'å¯¼å…¥å¤±è´¥: {str(e)}')
    
    return render_template('import.html')

@app.route('/statistics')
def statistics():
    """ç»Ÿè®¡é¡µé¢"""
    try:
        stats = paper_manager.get_statistics()
        return render_template('statistics.html', stats=stats)
    except Exception as e:
        print(f"ç»Ÿè®¡é¡µé¢é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return f"ç»Ÿè®¡é¡µé¢åŠ è½½é”™è¯¯: {e}", 500

@app.route('/api/categories')
def api_categories():
    """API: è·å–æ‰€æœ‰åˆ†ç±»"""
    conn = sqlite3.connect(paper_manager.db_path)
    cursor = conn.cursor()
    cursor.execute('SELECT DISTINCT category FROM papers WHERE category != "" ORDER BY category')
    categories = [row[0] for row in cursor.fetchall()]
    conn.close()
    return jsonify(categories)

@app.route('/api/venues')
def api_venues():
    """API: è·å–æ‰€æœ‰æœŸåˆŠ"""
    conn = sqlite3.connect(paper_manager.db_path)
    cursor = conn.cursor()
    cursor.execute('SELECT DISTINCT venue FROM papers WHERE venue != "" ORDER BY venue')
    venues = [row[0] for row in cursor.fetchall()]
    conn.close()
    return jsonify(venues)

@app.route('/api/search_suggestions')
def api_search_suggestions():
    """API: æœç´¢å»ºè®®"""
    query = request.args.get('q', '').strip()
    if not query or len(query) < 2:
        return jsonify([])
    
    conn = sqlite3.connect(paper_manager.db_path)
    cursor = conn.cursor()
    
    # æœç´¢æ ‡é¢˜å»ºè®®
    cursor.execute('''
        SELECT DISTINCT title_cn as title FROM papers 
        WHERE title_cn LIKE ? AND title_cn != ""
        UNION
        SELECT DISTINCT title_en as title FROM papers 
        WHERE title_en LIKE ? AND title_en != ""
        LIMIT 10
    ''', (f'%{query}%', f'%{query}%'))
    
    suggestions = [row[0] for row in cursor.fetchall()]
    conn.close()
    
    return jsonify(suggestions)

@app.route('/deduplication')
def deduplication_page():
    """å»é‡ç®¡ç†é¡µé¢"""
    stats = paper_manager.get_deduplication_stats()
    doi_duplicates = paper_manager.find_duplicate_papers()
    title_duplicates = paper_manager.find_duplicate_papers_by_title()
    
    return render_template('deduplication.html',
                         stats=stats,
                         doi_duplicates=doi_duplicates,
                         title_duplicates=title_duplicates)

@app.route('/api/auto-deduplicate', methods=['POST'])
def auto_deduplicate():
    """API: è‡ªåŠ¨å»é‡"""
    try:
        removed_count = paper_manager.auto_deduplicate_by_doi()
        return jsonify({
            'success': True,
            'message': f'æˆåŠŸåˆ é™¤ {removed_count} ç¯‡é‡å¤è®ºæ–‡',
            'removed_count': removed_count
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'å»é‡å¤±è´¥: {str(e)}'
        })

@app.route('/api/merge-papers', methods=['POST'])
def merge_papers():
    """API: åˆå¹¶è®ºæ–‡"""
    try:
        data = request.get_json()
        keep_id = data.get('keep_id')
        remove_id = data.get('remove_id')
        
        if not keep_id or not remove_id:
            return jsonify({
                'success': False,
                'message': 'ç¼ºå°‘å¿…è¦å‚æ•°'
            })
        
        success = paper_manager.merge_paper_data(keep_id, remove_id)
        
        if success:
            return jsonify({
                'success': True,
                'message': 'è®ºæ–‡åˆå¹¶æˆåŠŸ'
            })
        else:
            return jsonify({
                'success': False,
                'message': 'è®ºæ–‡åˆå¹¶å¤±è´¥'
            })
    
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'åˆå¹¶å¤±è´¥: {str(e)}'
        })

@app.route('/api/delete-paper/<int:paper_id>', methods=['DELETE'])
def delete_paper(paper_id):
    """API: åˆ é™¤è®ºæ–‡"""
    try:
        conn = sqlite3.connect(paper_manager.db_path)
        cursor = conn.cursor()
        
        cursor.execute('DELETE FROM papers WHERE id = ?', (paper_id,))
        
        if cursor.rowcount > 0:
            conn.commit()
            conn.close()
            return jsonify({
                'success': True,
                'message': 'è®ºæ–‡åˆ é™¤æˆåŠŸ'
            })
        else:
            conn.close()
            return jsonify({
                'success': False,
                'message': 'è®ºæ–‡ä¸å­˜åœ¨'
            })
    
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'åˆ é™¤å¤±è´¥: {str(e)}'
        })


if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨è®ºæ–‡ç®¡ç†ç³»ç»Ÿ")
    print("ğŸ“š è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:5001")
    print("ğŸ”§ ä½¿ç”¨Ctrl+Cåœæ­¢æœåŠ¡å™¨")
    
    app.run(debug=True, host='0.0.0.0', port=5001)
