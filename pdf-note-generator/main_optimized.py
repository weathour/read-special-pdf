import os
import sys
import argparse
import time
from pathlib import Path
from typing import List, Dict
import multiprocessing as mp

# è®¾ç½®ç¯å¢ƒå˜é‡æŠ‘åˆ¶verboseè¾“å‡º
os.environ['PDFMINER_LOG_LEVEL'] = 'ERROR'

from note_generator import NoteGenerator
from config import load_config, save_config
from file_manager import FileManager
from pdf_processor_optimized import OptimizedPDFProcessor

class OptimizedPDFNoteGenerator:
    """ä¼˜åŒ–çš„PDFç¬”è®°ç”Ÿæˆå™¨ï¼ˆåŒæ­¥ï¼Œä»…è¾“å‡ºç»“æ„åŒ– JSONï¼‰"""

    def __init__(self, config: Dict, max_workers: int = None):
        self.config = config
        self.file_manager = FileManager()
        self.config_hash = self.file_manager.calculate_config_hash(config)
        self.pdf_processor = OptimizedPDFProcessor(config)
        self.note_generator = NoteGenerator(config)
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'skipped_files': 0,
            'failed_files': 0,
            'start_time': time.time()
        }

    def process_directory(self, input_dir: Path, output_dir: Path,
                         force_reprocess: bool = False) -> Dict:
        """å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶"""

        print(f"ğŸ”„ æ‰«æPDFæ–‡ä»¶ | {str(input_dir)}")
        start_time = time.time()
        pdf_files = self._get_all_pdf_files(input_dir)
        end_time = time.time()
        print(f"âœ… æ‰«æPDFæ–‡ä»¶å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")

        if not pdf_files:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶")
            return self.stats

        self.stats['total_files'] = len(pdf_files)

        start_time = time.time()
        self._show_file_distribution(pdf_files, input_dir)
        end_time = time.time()
        print(f"âœ… æ–‡ä»¶åˆ†å¸ƒæ˜¾ç¤ºå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")

        if not force_reprocess:
            start_time = time.time()
            files_to_process = self._filter_files_to_process(pdf_files)
            end_time = time.time()
            print(f"âœ… æ–‡ä»¶è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")
            self.stats['skipped_files'] = len(pdf_files) - len(files_to_process)
        else:
            files_to_process = pdf_files

        if not files_to_process:
            print(f"â„¹ï¸ æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†è¿‡")
            return self.stats

        print(f"â„¹ï¸ éœ€è¦å¤„ç† {len(files_to_process)} ä¸ªæ–‡ä»¶")
        self._process_files_sequential(files_to_process, output_dir)
        self._show_final_stats()

        return self.stats

    def _get_all_pdf_files(self, directory: Path) -> List[Path]:
        """é€’å½’è·å–æ‰€æœ‰PDFæ–‡ä»¶"""
        pdf_files = []
        try:
            for pattern in ['*.pdf', '*.PDF']:
                pdf_files.extend(directory.rglob(pattern))
            pdf_files = sorted(list(set(pdf_files)))
            print(f"â„¹ï¸ æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
            return pdf_files
        except Exception as e:
            print(f"âŒ æ‰«ææ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return []

    def _show_file_distribution(self, pdf_files: List[Path], base_dir: Path):
        """æ˜¾ç¤ºæ–‡ä»¶åˆ†å¸ƒ"""
        if not pdf_files:
            return
        directories = {}
        total_size = 0
        for pdf_file in pdf_files:
            try:
                rel_path = pdf_file.relative_to(base_dir)
                parent_dir = str(rel_path.parent) if rel_path.parent != Path('.') else 'æ ¹ç›®å½•'
                if parent_dir not in directories:
                    directories[parent_dir] = {'count': 0, 'size': 0}
                file_size = pdf_file.stat().st_size
                directories[parent_dir]['count'] += 1
                directories[parent_dir]['size'] += file_size
                total_size += file_size
            except Exception:
                continue
        print(f"â„¹ï¸ æ–‡ä»¶åˆ†å¸ƒ (æ€»å¤§å°: {self._format_size(total_size)}):")
        for dir_name, info in sorted(directories.items()):
            size_str = self._format_size(info['size'])
            print(f"  ğŸ“ {dir_name}: {info['count']} ä¸ªæ–‡ä»¶ ({size_str})")

    def _filter_files_to_process(self, pdf_files: List[Path]) -> List[Path]:
        """è¿‡æ»¤éœ€è¦å¤„ç†çš„æ–‡ä»¶"""
        files_to_process = []
        for pdf_file in pdf_files:
            existing_record = self.file_manager.is_file_processed(pdf_file, self.config_hash)
            if not existing_record:
                files_to_process.append(pdf_file)
            elif existing_record.get('config_changed', False):
                print(f"âš ï¸ é…ç½®å·²æ›´æ”¹ï¼Œé‡æ–°å¤„ç†: {pdf_file.name}")
                files_to_process.append(pdf_file)
            elif existing_record.get('output_json_path') and not Path(existing_record['output_json_path']).exists():
                print(f"âš ï¸ è¾“å‡ºæ–‡ä»¶ä¸¢å¤±ï¼Œé‡æ–°å¤„ç†: {pdf_file.name}")
                files_to_process.append(pdf_file)
        return files_to_process

    def _process_files_sequential(self, pdf_files: List[Path], output_dir: Path):
        for i, pdf_file in enumerate(pdf_files, 1):
            print(f"ğŸš€ æ€»è¿›åº¦ {i}/{len(pdf_files)} | å¤„ç†ä¸­: {pdf_file.name}")
            try:
                start_time = time.time()
                self._process_single_file(pdf_file, output_dir)
                end_time = time.time()
                self.stats['processed_files'] += 1
                print(f"âœ… å¤„ç†å®Œæˆ: {pdf_file.name}ï¼Œæ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
            except Exception as e:
                self.stats['failed_files'] += 1
                print(f"âŒ å¤„ç†å¤±è´¥: {pdf_file.name} - {str(e)}")
                try:
                    self.file_manager.record_processing(
                        pdf_file, self.config_hash,
                        success=False, error_message=str(e)
                    )
                except Exception as db_e:
                    print(f"â— DBè®°å½•å¤„ç†å¤±è´¥: {db_e}")
                    
    def _process_single_file(self, pdf_file: Path, output_dir: Path):
        txt_dir = output_dir / "txt"
        pdf_dir = output_dir / "pdf"
        txt_dir.mkdir(exist_ok=True)
        pdf_dir.mkdir(exist_ok=True)

        # 1. æå–æ–‡æœ¬
        start = time.time()
        print(f"ğŸ”„ æå–æ–‡æœ¬: {pdf_file.name}")
        text = self.pdf_processor.extract_text(pdf_file)
        if not text or not text.strip():
            raise Exception("æ— æ³•æå–æ–‡æœ¬å†…å®¹")
        txt_file_path = txt_dir / f"{pdf_file.stem}.txt"
        with open(txt_file_path, 'w', encoding='utf-8') as txt_file:
            txt_file.write(text)
        print(f"âœ… æå–çš„æ–‡æœ¬å·²ä¿å­˜åˆ°: {txt_file_path}ï¼Œè€—æ—¶: {time.time() - start:.2f} ç§’")

        # 2. æ–‡æœ¬åˆ†å—
        start = time.time()
        print(f"ğŸ”„ æ–‡æœ¬åˆ†å—: {pdf_file.name}")
        chunks = self.pdf_processor.chunk_text(text)
        print(f"âœ… æ–‡æœ¬åˆ†å—å®Œæˆï¼Œè€—æ—¶: {time.time() - start:.2f} ç§’")

        # 3. ä¿¡æ¯æå–
        start = time.time()
        print(f"ğŸ”„ ä¿¡æ¯æå–: {pdf_file.name}")
        structured_info = self.note_generator.extract_structured_info(chunks)
        print(f"âœ… ä¿¡æ¯æå–å®Œæˆï¼Œè€—æ—¶: {time.time() - start:.2f} ç§’")

        # 4. ä¿å­˜ç»“æ„åŒ–ä¿¡æ¯
        start = time.time()
        print(f"ğŸ”„ ä¿å­˜ç»“æ„åŒ–ä¿¡æ¯: {pdf_file.name}")
        self.note_generator.save_structured_info(pdf_file, structured_info, output_dir)
        print(f"âœ… ç»“æ„åŒ–ä¿¡æ¯ä¿å­˜å®Œæˆï¼Œè€—æ—¶: {time.time() - start:.2f} ç§’")

        # 5. å¤åˆ¶PDF
        start = time.time()
        try:
            pdf_copy_path = pdf_dir / pdf_file.name
            import shutil
            shutil.copy(pdf_file, pdf_copy_path)
            print(f"âœ… PDF æ–‡ä»¶å·²å¤åˆ¶åˆ°: {pdf_copy_path}ï¼Œè€—æ—¶: {time.time() - start:.2f} ç§’")
        except Exception as e:
            print(f"âŒ å¤åˆ¶ PDF æ–‡ä»¶å¤±è´¥: {str(e)}")

        processing_time = time.time()
        base_name = pdf_file.stem
        json_files = list((output_dir / "json").glob(f"{base_name}_*.json"))

        self.file_manager.record_processing(
            pdf_file, self.config_hash,
            output_json_path=json_files[-1] if json_files else None,
            processing_time=processing_time,
            success=True
        )

    def _format_size(self, size):
        if size < 1024:
            return f"{size} B"
        elif size < 1024 * 1024:
            return f"{size / 1024:.1f} KB"
        elif size < 1024 * 1024 * 1024:
            return f"{size / (1024 * 1024):.1f} MB"
        else:
            return f"{size / (1024 * 1024 * 1024):.1f} GB"

    def _show_final_stats(self):
        end_time = time.time()
        total_time = end_time - self.stats['start_time']
        print(f"\n{'='*60}")
        print(f"ğŸ‰ å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
        print(f"   æˆåŠŸå¤„ç†: {self.stats['processed_files']}")
        print(f"   è·³è¿‡æ–‡ä»¶: {self.stats['skipped_files']}")
        print(f"   å¤±è´¥æ–‡ä»¶: {self.stats['failed_files']}")
        print(f"   æˆåŠŸç‡: {(self.stats['processed_files'] / self.stats['total_files'] * 100):.1f}%")
        print(f"   æ€»è€—æ—¶: {total_time:.1f} ç§’")

def main():
    parser = argparse.ArgumentParser(description='PDF Paper Structured JSON Extractor (Sync)')
    parser.add_argument('--input-dir', '-i', required=True, help='PDF input directory')
    parser.add_argument('--output-dir', '-o', default='./output', help='Output directory')
    parser.add_argument('--config', '-c', default='./config.json', help='Config file path')
    parser.add_argument('--force', '-f', action='store_true', help='Force re-process all files')
    parser.add_argument('--quiet', '-q', action='store_true', help='Quiet mode, less output')
    parser.add_argument('--workers', '-w', type=int, help='Number of workers (not used for async, kept for compatibility)')
    parser.add_argument('--pdf-method', choices=['auto', 'pymupdf', 'pdfplumber', 'pypdf2', 'pdfminer'],
                        default='auto', help='PDF processing method')

    # ç®¡ç†å‘½ä»¤
    parser.add_argument('--stats', action='store_true', help='Show processing statistics')
    parser.add_argument('--list-history', action='store_true', help='Show processing history')
    parser.add_argument('--cleanup-old', type=int, metavar='DAYS', help='Cleanup failed records older than N days')

    args = parser.parse_args()

    if args.stats or args.list_history or args.cleanup_old:
        file_manager = FileManager()
        if args.stats:
            stats = file_manager.get_statistics()
            print("ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"   æ€»å¤„ç†æ•°é‡: {stats['total_processed']}")
            print(f"   æˆåŠŸå¤„ç†: {stats['successful_processed']}")
            print(f"   å¤±è´¥å¤„ç†: {stats['failed_processed']}")
            print(f"   æˆåŠŸç‡: {stats['success_rate']:.1f}%")
            print(f"   æ€»å¤„ç†æ—¶é—´: {stats['total_time']:.1f}s")
            print(f"   å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_time']:.1f}s")
        if args.list_history:
            history = file_manager.get_processing_history(20)
            print("\nğŸ“‹ æœ€è¿‘å¤„ç†è®°å½•:")
            for record in history:
                status = "âœ…" if record['success'] else "âŒ"
                print(f"{status} {record['file_name']} - {record['processed_at']}")
        if args.cleanup_old:
            count = file_manager.cleanup_old_records(args.cleanup_old)
            print(f"âœ… æ¸…ç†äº† {count} æ¡å¤±è´¥è®°å½•")
        return

    input_path = Path(args.input_dir)
    if not input_path.exists():
        print(f"âŒ é”™è¯¯ï¼šè¾“å…¥ç›®å½• {input_path} ä¸å­˜åœ¨")
        sys.exit(1)

    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    (output_path / "json").mkdir(exist_ok=True)
    (output_path / "txt").mkdir(exist_ok=True)
    (output_path / "pdf").mkdir(exist_ok=True)

    config = load_config(args.config)
    if args.pdf_method != 'auto':
        config['processing']['pdf_method'] = args.pdf_method
    max_workers = args.workers or min(4, mp.cpu_count())

    processor = OptimizedPDFNoteGenerator(config, max_workers=max_workers)

    try:
        processor.process_directory(
            input_path,
            output_path,
            force_reprocess=args.force
        )
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()